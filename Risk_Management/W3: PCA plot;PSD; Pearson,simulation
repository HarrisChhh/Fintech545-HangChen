Q1: Use PCA and plot the cumulative variance 
A1:
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from google.colab import drive
drive.mount('/content/drive')

Option_file = '/content/drive/MyDrive/Colab Notebooks/AAPL_Options.csv'
data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/DailyReturn.csv')

# Exponentially Weighted Covariance Function
def exponentially_weighted_covariance(X, lambd):
    T, N = X.shape
    weights = lambd ** np.arange(T - 1, -1, -1)
    weights /= weights.sum()
    mu = np.average(X, axis=0, weights=weights)
    X_centered = X - mu
    return (X_centered * np.sqrt(weights[:, None])).T @ (X_centered * np.sqrt(weights[:, None]))

# Compute PCA for Multiple Lambda Values
lambdas = [0.999, 0.99, 0.9, 0.7, 0.5, 0.3]
cumulative_variances = {}

for lambd in lambdas:
    Sigma = exponentially_weighted_covariance(data.values, lambd)
    pca = PCA().fit(Sigma)
    cumulative_variances[lambd] = np.cumsum(pca.explained_variance_ratio_)

# Plot Results
plt.figure(figsize=(10, 6))
for lambd, cum_var in cumulative_variances.items():
    plt.plot(range(1, len(cum_var) + 1), cum_var, label=f'Î» = {lambd}')
plt.title('Cumulative Variance Explained by PCA')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Variance Explained')
plt.legend()
plt.grid()





Q2: PSD;non-PSD;near_psd;Higham psd;Compare the results of both using the Frobenius Norm
A2:
import numpy as np
import time

# Functions for PSD Matrix Computation
def near_psd(A, epsilon=0):
    """Nearest PSD Matrix using Eigenvalue Adjustment"""
    A_sym = (A + A.T) / 2
    eigvals, eigvecs = np.linalg.eigh(A_sym)
    eigvals[eigvals < epsilon] = epsilon
    return eigvecs @ np.diag(eigvals) @ eigvecs.T

def higham_nearest_psd(A, max_iter=100, tol=1e-8):
    """Higham's Nearest PSD Method"""
    Y, delta_S = A.copy(), np.zeros_like(A)
    for _ in range(max_iter):
        R = Y - delta_S
        X = (R + R.T) / 2
        eigvals, eigvecs = np.linalg.eigh(X)
        eigvals[eigvals < 0] = 0
        X_psd = eigvecs @ np.diag(eigvals) @ eigvecs.T
        delta_S = X_psd - R
        Y = np.diag(np.ones(len(A))) + X_psd - np.diag(X_psd.diagonal())
        if np.linalg.norm(Y - X_psd, 'fro') < tol:
            break
    return Y

def generate_non_psd_matrix(n):
    """Generate a Non-PSD Correlation Matrix"""
    A = np.random.randn(n, n)
    A = (A + A.T) / 2
    np.fill_diagonal(A, 1)
    A[0, 1] = A[1, 0] = 2
    return A

def is_psd(A):
    """Check if Matrix is PSD"""
    return np.all(np.linalg.eigvalsh(A) >= 0)

# Compare Methods for Scaling
def compare_psd_methods(matrix_sizes):
    results = {"near_psd": [], "higham_psd": []}
    for n in matrix_sizes:
        print(f"\nMatrix size: {n}x{n}")
        A = generate_non_psd_matrix(n)
        print("Is original matrix PSD?", is_psd(A))
        
        # near_psd
        start = time.time()
        A_near = near_psd(A)
        results["near_psd"].append(time.time() - start)
        print("Is near_psd PSD?", is_psd(A_near))
        
        # Higham's method
        start = time.time()
        A_higham = higham_nearest_psd(A)
        results["higham_psd"].append(time.time() - start)
        print("Is Higham PSD?", is_psd(A_higham))
        
        # Frobenius norms
        print(f"Frobenius norm (near_psd): {np.linalg.norm(A - A_near, 'fro'):.4f}")
        print(f"Frobenius norm (Higham): {np.linalg.norm(A - A_higham, 'fro'):.4f}")
    return results

# Run Comparison
matrix_sizes = [100, 200, 300, 400, 500]
timings = compare_psd_methods(matrix_sizes)

# Display Timings
print("\nTimings (seconds):")
for method, times in timings.items():
    print(f"{method}: {times}")





Q3: Pearson correlation + var()), Pearson correlation + EW variance; simulation with PCA
A3:

import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from numpy.linalg import norm
import time

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/DailyReturn.csv')

# Functions for Exponentially Weighted Covariance and Correlation
def exponentially_weighted_covariance(X, lambd):
    T = X.shape[0]
    weights = lambd ** np.arange(T - 1, -1, -1)
    weights /= weights.sum()
    X_centered = X - np.average(X, axis=0, weights=weights)
    return (X_centered * weights[:, None]).T @ X_centered

def exponentially_weighted_correlation(X, lambd):
    ew_cov = exponentially_weighted_covariance(X, lambd)
    std = np.sqrt(np.diag(ew_cov))
    return np.clip(ew_cov / np.outer(std, std), -1, 1)

def construct_covariance(corr_matrix, variances):
    std_dev = np.sqrt(variances)
    return corr_matrix * np.outer(std_dev, std_dev)

# Functions for Simulations
def simulate_direct(cov_matrix, n_samples):
    mean_vector = np.zeros(cov_matrix.shape[0])
    return np.random.multivariate_normal(mean_vector, cov_matrix, n_samples)

def simulate_pca(cov_matrix, n_samples, variance_explained):
    eigvals, eigvecs = np.linalg.eigh(cov_matrix)
    idx = np.argsort(eigvals)[::-1]
    eigvals, eigvecs = eigvals[idx], eigvecs[:, idx]
    cum_variance = np.cumsum(eigvals) / eigvals.sum()
    num_components = np.searchsorted(cum_variance, variance_explained) + 1
    selected_eigvals = eigvals[:num_components]
    selected_eigvecs = eigvecs[:, :num_components]
    random_samples = np.random.randn(n_samples, num_components)
    return random_samples @ np.diag(np.sqrt(selected_eigvals)) @ selected_eigvecs.T

# Parameters
lambd = 0.97
n_samples = 25000
variance_explained_levels = [1.0, 0.75, 0.50]

# Compute Pearson and Exponentially Weighted Matrices
pearson_corr = np.corrcoef(data, rowvar=False)
standard_variance = np.var(data, axis=0)
ew_corr = exponentially_weighted_correlation(data, lambd)
ew_variance = np.diag(exponentially_weighted_covariance(data, lambd))

cov_matrices = {
    'pearson_var': construct_covariance(pearson_corr, standard_variance),
    'ew_corr_ew_var': construct_covariance(ew_corr, ew_variance)
}

# Simulations and Timings
results, timings = {}, {}
for name, cov_matrix in cov_matrices.items():
    print(f"\nSimulating from {name} matrix")
    results[name], timings[name] = {}, {}

    # Direct Simulation
    start = time.time()
    results[name]['direct'] = simulate_direct(cov_matrix, n_samples)
    timings[name]['direct'] = time.time() - start

    # PCA Simulation
    for level in variance_explained_levels:
        start = time.time()
        results[name][f'pca_{int(level * 100)}'] = simulate_pca(cov_matrix, n_samples, level)
        timings[name][f'pca_{int(level * 100)}'] = time.time() - start

# Compare Simulated vs Input Covariances
frobenius_norms = {}
for name, cov_matrix in cov_matrices.items():
    frobenius_norms[name] = {}
    for method, sim_data in results[name].items():
        sim_cov = np.cov(sim_data, rowvar=False)
        frobenius_norms[name][method] = norm(cov_matrix - sim_cov, 'fro')

# Print Results
print("\nTimings (seconds):")
for name, methods in timings.items():
    print(f"{name}:")
    for method, time_ in methods.items():
        print(f"  {method}: {time_:.2f} seconds")

print("\nFrobenius Norms:")
for name, methods in frobenius_norms.items():
    print(f"{name}:")
    for method, fro_norm in methods.items():
        print(f"  {method}: {fro_norm:.4f}")
 
